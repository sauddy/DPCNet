{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DPCNet-V1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Dev_DPNNET",
      "language": "python",
      "name": "dev_dpnnet"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauddy/DPCNet/blob/main/DPCNet_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Av3XMdEU-5_"
      },
      "source": [
        "## DPCNet_Build -- 26 November  2020 Colab Compatible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG8TXKEFVeok",
        "outputId": "11813f4e-6a83-4545-a364-602ff1c72418"
      },
      "source": [
        "## Please note this version of the code is compatible with Google colab and will also run on Local Computer ## \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3CDNlzfU-6A"
      },
      "source": [
        "                           ##### IDEA Behind this notebook : ###########\n",
        "## Author : Sayantan \n",
        "## Created : 26 November 2020\n",
        "## This notebook is adopted from the CNN_DPNNet V3 and V5\n",
        "## CNN modules, RESNET50, ALEXNET, VGG16 are implemented by Ramit Dey\n",
        "\n",
        "\n",
        "\n",
        "## The primary idea is to develop a modular notebook that does the following:\n",
        "## Import all the customized Modules from Modules_DPCNet \n",
        "## For data processing use data_processing.py\n",
        "## A function module to call the different networks independently. (deep_models.py, other_cnn.py)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyZWiknOU-6A",
        "outputId": "9e96f1c3-a86d-4901-843d-cb9e464039ab"
      },
      "source": [
        "# import the necessary packages\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "## Modules to check the performance of the code\n",
        "from time import process_time \n",
        "!pip install memory_profiler ## When running from Google Colab\n",
        "import memory_profiler as mem_profile\n",
        "print('Memory (Before): {}Mb'.format(mem_profile.memory_usage()))\n",
        "\n",
        "\n",
        "## Importing the necessary TesnorFLow modules modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import r2_score ## form calcualting the r2 score\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: memory_profiler in /usr/local/lib/python3.6/dist-packages (0.58.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from memory_profiler) (5.4.8)\n",
            "Memory (Before): [123.59375]Mb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9468NQc2-Cq",
        "outputId": "2cfa6075-e8e1-4958-f147-608d4e4e6ae0"
      },
      "source": [
        "############ Please provide the same path to the code directory if using Colab################\n",
        "\n",
        "Path_gdrive= '/content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/' ## Comment out this line if using local computer\n",
        "\n",
        "## Importing the Modules from Modules_DPCNet\n",
        "import sys\n",
        "try: ## tries to find the modules in the local directory first\n",
        "  path ='' # For local computer  \n",
        "  sys.path.append(path+'MODULES_DPCNeT')\n",
        "  import data_processing as dp\n",
        "  import deep_models as dm\n",
        "  import other_cnns as ocn\n",
        "\n",
        "  ########### Folders to save the processed data, files and figures when using Local computer ##############\n",
        "  !mkdir -p data_folder\n",
        "  !mkdir -p figures ## to save the figurs\n",
        "  !mkdir -p saved_model\n",
        "  \n",
        "except ModuleNotFoundError:\n",
        "  \n",
        "  # #For Colab use:\n",
        "  # #Point to the path containing the modules in the above section\n",
        "  #(data folder are a directory above the directory containing the notebook)\n",
        "  try:\n",
        "    path =Path_gdrive\n",
        "    print(path)\n",
        "    sys.path.append(path+'MODULES_DPCNeT')\n",
        "    import data_processing as dp\n",
        "    import deep_models as dm\n",
        "    import other_cnns as ocn\n",
        "\n",
        "    ########### Folders to save the processed data, files and figures when using GDRIVE ##############\n",
        "    import os\n",
        "    os.chdir(path)\n",
        "    print(\"Creating the folders\")\n",
        "    !mkdir -p data_folder\n",
        "    !mkdir -p figures ## to save the figurs\n",
        "    !mkdir -p figures_paper\n",
        "    !mkdir -p saved_model\n",
        "  except ModuleNotFoundError:\n",
        "    print(\"The path to the modules is incorrect-- Provide current path\")\n",
        "\n",
        "print(\"[INFO] Modules imported\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/\n",
            "We are currently using the Modules_DPCNet\n",
            "Creating the folders\n",
            "[INFO] Modules imported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIBEGqX9hTOE"
      },
      "source": [
        "# ########### Folders to save the process data, files and figures when using GDRIVE ##############\n",
        "\n",
        "# import os\n",
        "# os.chdir(Path_gdrive)\n",
        "# !mkdir -p data_folder\n",
        "# !mkdir -p figures ## to save the figurs\n",
        "# !mkdir -p figures_paper\n",
        "# !mkdir -p saved_model\n",
        "\n",
        "# ########### Folders to save the process data, files and figures when using Local computer ##############\n",
        "# # !mkdir -p data_folder\n",
        "# # !mkdir -p figures ## to save the figurs\n",
        "# # !mkdir -p saved_model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldbzXokMU-6B"
      },
      "source": [
        "## Load data csv (including the path to images) from multiple time-instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJh7NLv7U-6B",
        "outputId": "57dc5abd-ef7d-46fd-c780-a3e18a95a56b"
      },
      "source": [
        "############# Address to the data folder ###################\n",
        "list_of_orbits = ['150','140','130','120']\n",
        "\n",
        "print(\"[INFO]: Importing files from the datafolder\")\n",
        "## Note the data folder are a directory above the code directory\n",
        "folder_address = path + \"../analysis_output_\"\n",
        "# folder_address = \"/content/drive/MyDrive/CNN_DPNNET/analysis_output_\" \n",
        "# The idea is to generate a dataframe with the parameters and the path to the images\n",
        "data_complete = dp.parse_time_series_data(folder_address,list_of_orbits,path)\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Importing files from the datafolder\n",
            "[INFO] preparing the dataframe from differnt times...\n",
            "Reading the image paths and data from folder: /content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/../analysis_output_150/\n",
            "Reading the image paths and data from folder: /content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/../analysis_output_140/\n",
            "Reading the image paths and data from folder: /content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/../analysis_output_130/\n",
            "Reading the image paths and data from folder: /content/drive/MyDrive/CNN_DPNNET/DPCNET_dev/../analysis_output_120/\n",
            "[INFO] The concatination of dataframes from differnt times are now complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDxiZTS9U-6B"
      },
      "source": [
        "## Preparing data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agTa7GkoU-6B",
        "outputId": "529bb2c4-7efb-46ca-ef4f-1a20846db7a5"
      },
      "source": [
        "## partition the data csv file into training and testing splits using 85% of\n",
        "## the data for training and the remaining 15% for testing\n",
        "split = train_test_split(data_complete, test_size=0.15, random_state=42)\n",
        "(train, test) = split\n",
        "## Save the train and the test data for future use as well.\n",
        "\n",
        "test.to_csv(path+'data_folder/test_dataset.csv')\n",
        "train.to_csv(path+'data_folder/train_dataset.csv')\n",
        "\n",
        "## Generate the Normalized data\n",
        "normed_train_data, normed_test_data, train_labels, test_labels = dp.process_the_disk_attributes(train, test, path)\n",
        "\n",
        "\n",
        "#### Desired Image resoltuion #####\n",
        "X_res = Y_res = 128\n",
        "\n",
        "## Generate the training and the test images \n",
        "\n",
        "trainImagesX = dp.load_disk_images(train, X_res, Y_res, Type = \"Train\")\n",
        "testImagesX = dp.load_disk_images(test, X_res, Y_res, Type = \"Test\")\n",
        "\n",
        "Validation_split = 0.15 # 15 percent of the training data is used for validation\n",
        "print('Memory (After Loading): {}Mb'.format(mem_profile.memory_usage()))\n",
        "print('There are {} Train, {} Validation and {} Test images'.format(int((1-Validation_split)*len(normed_train_data)),int(Validation_split*len(normed_train_data)),len(normed_test_data)))## check the numbers in each chategory\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] preparing the normalized data training/testing split...\n",
            "[INFO] Done...\n",
            "[INFO] Loading images from Train data..\n",
            "Train Images are loaded\n",
            "[INFO] Loading images from Test data..\n",
            "Test Images are loaded\n",
            "Memory (After Loading): [628.0390625]Mb\n",
            "There are 2128 Train, 375 Validation and 442 Test images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYGYS3pgU-6C"
      },
      "source": [
        "## Training the CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkIRL4NmU-6C"
      },
      "source": [
        "## Hyper-Parameter to define\n",
        "batch_size = 20 ## the best was for 200 last run\n",
        "valid_batch_size = 20\n",
        "epochs=200 ## best was 100\n",
        "init_lr = 1e-5\n",
        "\n",
        "# early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.2, patience=20, verbose=1, mode='min',restore_best_weights=True)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lW02M4WU-6C",
        "outputId": "008db85c-b315-4809-bd58-36006d6e7d1c"
      },
      "source": [
        "## Select the Network type\n",
        "\n",
        "# NETWORK = \"Vanilla\"\n",
        "# NETWORK = \"ALEXNET\"\n",
        "# NETWORK = \"VGG\"\n",
        "NETWORK = \"RESNET50\"\n",
        "\n",
        "print('INFO: Currently training using the {} NETWORK'.format(NETWORK))\n",
        "if NETWORK == \"Vanilla\":\n",
        "    CNN = dm.build_cnn(X_res, Y_res, 3, regress=True)\n",
        "elif NETWORK == \"ALEXNET\":\n",
        "    CNN = ocn.alexnet(X_res, Y_res, 3, regress=True)\n",
        "elif NETWORK == \"VGG\":\n",
        "    CNN = ocn.cnn_vgg(X_res, Y_res, 3, regress=True)\n",
        "elif NETWORK == \"RESNET50\":\n",
        "    CNN = ocn.ResNet50(X_res, Y_res, 3)\n",
        "# optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(init_lr, decay=init_lr/2000)\n",
        "CNN.compile(loss='mean_squared_error',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "CNN_history = CNN.fit(x=trainImagesX, y=train_labels,\n",
        "                  validation_split = 0.15,epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n",
        "print('Memory (After Training): {}Mb'.format(mem_profile.memory_usage()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Currently training using the RESNET50 NETWORK\n",
            "Epoch 1/200\n",
            "107/107 [==============================] - 19s 173ms/step - loss: 2525.8181 - mean_absolute_error: 41.8694 - mean_squared_error: 2525.8181 - val_loss: 5843.9316 - val_mean_absolute_error: 70.5167 - val_mean_squared_error: 5843.9316\n",
            "Epoch 2/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 813.1365 - mean_absolute_error: 24.1773 - mean_squared_error: 813.1365 - val_loss: 4088.1951 - val_mean_absolute_error: 57.0426 - val_mean_squared_error: 4088.1951\n",
            "Epoch 3/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 613.3069 - mean_absolute_error: 20.5633 - mean_squared_error: 613.3069 - val_loss: 2032.1571 - val_mean_absolute_error: 38.4388 - val_mean_squared_error: 2032.1571\n",
            "Epoch 4/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 456.3993 - mean_absolute_error: 17.5253 - mean_squared_error: 456.3993 - val_loss: 709.9125 - val_mean_absolute_error: 22.1957 - val_mean_squared_error: 709.9125\n",
            "Epoch 5/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 309.8557 - mean_absolute_error: 13.9860 - mean_squared_error: 309.8557 - val_loss: 293.1843 - val_mean_absolute_error: 13.5463 - val_mean_squared_error: 293.1843\n",
            "Epoch 6/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 216.5295 - mean_absolute_error: 11.4386 - mean_squared_error: 216.5295 - val_loss: 224.7654 - val_mean_absolute_error: 11.7772 - val_mean_squared_error: 224.7655\n",
            "Epoch 7/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 167.3323 - mean_absolute_error: 10.0172 - mean_squared_error: 167.3323 - val_loss: 179.4366 - val_mean_absolute_error: 10.2173 - val_mean_squared_error: 179.4366\n",
            "Epoch 8/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 131.7003 - mean_absolute_error: 8.8013 - mean_squared_error: 131.7003 - val_loss: 159.6072 - val_mean_absolute_error: 9.4979 - val_mean_squared_error: 159.6072\n",
            "Epoch 9/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 120.9574 - mean_absolute_error: 8.3864 - mean_squared_error: 120.9574 - val_loss: 147.1488 - val_mean_absolute_error: 9.1292 - val_mean_squared_error: 147.1488\n",
            "Epoch 10/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 105.3362 - mean_absolute_error: 7.9428 - mean_squared_error: 105.3362 - val_loss: 137.2037 - val_mean_absolute_error: 8.6189 - val_mean_squared_error: 137.2037\n",
            "Epoch 11/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 81.8690 - mean_absolute_error: 6.8873 - mean_squared_error: 81.8690 - val_loss: 138.0110 - val_mean_absolute_error: 8.9739 - val_mean_squared_error: 138.0110\n",
            "Epoch 12/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 81.1112 - mean_absolute_error: 6.9788 - mean_squared_error: 81.1112 - val_loss: 145.4730 - val_mean_absolute_error: 9.0372 - val_mean_squared_error: 145.4730\n",
            "Epoch 13/200\n",
            "107/107 [==============================] - 17s 155ms/step - loss: 72.1185 - mean_absolute_error: 6.5609 - mean_squared_error: 72.1185 - val_loss: 155.5089 - val_mean_absolute_error: 9.7010 - val_mean_squared_error: 155.5089\n",
            "Epoch 14/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 58.6283 - mean_absolute_error: 5.9280 - mean_squared_error: 58.6283 - val_loss: 109.4068 - val_mean_absolute_error: 7.5542 - val_mean_squared_error: 109.4068\n",
            "Epoch 15/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 72.2222 - mean_absolute_error: 6.4449 - mean_squared_error: 72.2222 - val_loss: 117.7910 - val_mean_absolute_error: 7.8207 - val_mean_squared_error: 117.7910\n",
            "Epoch 16/200\n",
            "107/107 [==============================] - 17s 155ms/step - loss: 59.7642 - mean_absolute_error: 6.0011 - mean_squared_error: 59.7642 - val_loss: 124.5475 - val_mean_absolute_error: 8.1960 - val_mean_squared_error: 124.5475\n",
            "Epoch 17/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 54.2610 - mean_absolute_error: 5.6417 - mean_squared_error: 54.2610 - val_loss: 158.8354 - val_mean_absolute_error: 9.8015 - val_mean_squared_error: 158.8354\n",
            "Epoch 18/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 53.3123 - mean_absolute_error: 5.7339 - mean_squared_error: 53.3123 - val_loss: 111.5032 - val_mean_absolute_error: 7.5948 - val_mean_squared_error: 111.5032\n",
            "Epoch 19/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 52.2250 - mean_absolute_error: 5.5255 - mean_squared_error: 52.2250 - val_loss: 150.5040 - val_mean_absolute_error: 9.3321 - val_mean_squared_error: 150.5040\n",
            "Epoch 20/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 42.7757 - mean_absolute_error: 5.0053 - mean_squared_error: 42.7757 - val_loss: 122.2685 - val_mean_absolute_error: 8.2774 - val_mean_squared_error: 122.2685\n",
            "Epoch 21/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 45.4572 - mean_absolute_error: 5.1235 - mean_squared_error: 45.4572 - val_loss: 105.6950 - val_mean_absolute_error: 7.4561 - val_mean_squared_error: 105.6950\n",
            "Epoch 22/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 38.2500 - mean_absolute_error: 4.7332 - mean_squared_error: 38.2500 - val_loss: 100.9971 - val_mean_absolute_error: 7.2370 - val_mean_squared_error: 100.9971\n",
            "Epoch 23/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 37.2889 - mean_absolute_error: 4.7246 - mean_squared_error: 37.2889 - val_loss: 90.8108 - val_mean_absolute_error: 6.9182 - val_mean_squared_error: 90.8108\n",
            "Epoch 24/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 37.6066 - mean_absolute_error: 4.7140 - mean_squared_error: 37.6066 - val_loss: 97.3490 - val_mean_absolute_error: 7.1598 - val_mean_squared_error: 97.3490\n",
            "Epoch 25/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 35.8160 - mean_absolute_error: 4.6136 - mean_squared_error: 35.8160 - val_loss: 96.8882 - val_mean_absolute_error: 7.0463 - val_mean_squared_error: 96.8882\n",
            "Epoch 26/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 34.0461 - mean_absolute_error: 4.5480 - mean_squared_error: 34.0461 - val_loss: 92.2966 - val_mean_absolute_error: 6.8400 - val_mean_squared_error: 92.2966\n",
            "Epoch 27/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 33.1004 - mean_absolute_error: 4.4462 - mean_squared_error: 33.1004 - val_loss: 84.3634 - val_mean_absolute_error: 6.4231 - val_mean_squared_error: 84.3634\n",
            "Epoch 28/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 31.3432 - mean_absolute_error: 4.2851 - mean_squared_error: 31.3432 - val_loss: 104.5609 - val_mean_absolute_error: 7.5106 - val_mean_squared_error: 104.5609\n",
            "Epoch 29/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 30.1892 - mean_absolute_error: 4.2198 - mean_squared_error: 30.1892 - val_loss: 85.9111 - val_mean_absolute_error: 6.5410 - val_mean_squared_error: 85.9111\n",
            "Epoch 30/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 28.7887 - mean_absolute_error: 4.0804 - mean_squared_error: 28.7887 - val_loss: 87.6067 - val_mean_absolute_error: 6.5865 - val_mean_squared_error: 87.6067\n",
            "Epoch 31/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 32.0024 - mean_absolute_error: 4.3643 - mean_squared_error: 32.0024 - val_loss: 100.7010 - val_mean_absolute_error: 7.1489 - val_mean_squared_error: 100.7010\n",
            "Epoch 32/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 28.1125 - mean_absolute_error: 4.1201 - mean_squared_error: 28.1125 - val_loss: 92.5248 - val_mean_absolute_error: 6.7097 - val_mean_squared_error: 92.5248\n",
            "Epoch 33/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 27.0894 - mean_absolute_error: 4.0208 - mean_squared_error: 27.0894 - val_loss: 95.6823 - val_mean_absolute_error: 6.7923 - val_mean_squared_error: 95.6823\n",
            "Epoch 34/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 29.4273 - mean_absolute_error: 4.1915 - mean_squared_error: 29.4273 - val_loss: 83.1414 - val_mean_absolute_error: 6.3434 - val_mean_squared_error: 83.1414\n",
            "Epoch 35/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 30.3723 - mean_absolute_error: 4.1144 - mean_squared_error: 30.3723 - val_loss: 83.5846 - val_mean_absolute_error: 6.2493 - val_mean_squared_error: 83.5846\n",
            "Epoch 36/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 27.3561 - mean_absolute_error: 4.0653 - mean_squared_error: 27.3561 - val_loss: 88.6277 - val_mean_absolute_error: 6.6890 - val_mean_squared_error: 88.6277\n",
            "Epoch 37/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 23.4022 - mean_absolute_error: 3.7392 - mean_squared_error: 23.4022 - val_loss: 86.8650 - val_mean_absolute_error: 6.3816 - val_mean_squared_error: 86.8650\n",
            "Epoch 38/200\n",
            "107/107 [==============================] - 17s 158ms/step - loss: 24.2097 - mean_absolute_error: 3.7460 - mean_squared_error: 24.2097 - val_loss: 82.5550 - val_mean_absolute_error: 6.6718 - val_mean_squared_error: 82.5550\n",
            "Epoch 39/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 24.6752 - mean_absolute_error: 3.7777 - mean_squared_error: 24.6752 - val_loss: 77.7684 - val_mean_absolute_error: 6.2350 - val_mean_squared_error: 77.7684\n",
            "Epoch 40/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 20.3938 - mean_absolute_error: 3.4941 - mean_squared_error: 20.3938 - val_loss: 87.3142 - val_mean_absolute_error: 6.8172 - val_mean_squared_error: 87.3142\n",
            "Epoch 41/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 23.6373 - mean_absolute_error: 3.6421 - mean_squared_error: 23.6373 - val_loss: 97.6769 - val_mean_absolute_error: 7.2155 - val_mean_squared_error: 97.6770\n",
            "Epoch 42/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 20.5772 - mean_absolute_error: 3.5309 - mean_squared_error: 20.5772 - val_loss: 85.3591 - val_mean_absolute_error: 6.5352 - val_mean_squared_error: 85.3591\n",
            "Epoch 43/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 21.2157 - mean_absolute_error: 3.5266 - mean_squared_error: 21.2157 - val_loss: 73.7077 - val_mean_absolute_error: 5.9468 - val_mean_squared_error: 73.7077\n",
            "Epoch 44/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 21.0696 - mean_absolute_error: 3.4904 - mean_squared_error: 21.0696 - val_loss: 79.8979 - val_mean_absolute_error: 6.1666 - val_mean_squared_error: 79.8979\n",
            "Epoch 45/200\n",
            "107/107 [==============================] - 17s 157ms/step - loss: 19.0021 - mean_absolute_error: 3.3258 - mean_squared_error: 19.0021 - val_loss: 72.2477 - val_mean_absolute_error: 5.7001 - val_mean_squared_error: 72.2477\n",
            "Epoch 46/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 17.8082 - mean_absolute_error: 3.2615 - mean_squared_error: 17.8082 - val_loss: 78.0876 - val_mean_absolute_error: 6.0425 - val_mean_squared_error: 78.0876\n",
            "Epoch 47/200\n",
            "107/107 [==============================] - 17s 156ms/step - loss: 18.1375 - mean_absolute_error: 3.2860 - mean_squared_error: 18.1375 - val_loss: 74.8559 - val_mean_absolute_error: 5.8161 - val_mean_squared_error: 74.8559\n",
            "Epoch 48/200\n",
            " 53/107 [=============>................] - ETA: 7s - loss: 18.6002 - mean_absolute_error: 3.3299 - mean_squared_error: 18.6002"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m9VZnHrU-6C"
      },
      "source": [
        "## The plots for the Validation and the Testing loss\n",
        "dp.plot_history(CNN_history,path, Model = \"CNN\")\n",
        "hist_df = pd.DataFrame(CNN_history.history)  ## converting to dataframe\n",
        "## Saving the history\n",
        "hist_df.to_csv(path+'data_folder/'+NETWORK+'_history.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTSZux-jU-6C"
      },
      "source": [
        "## Saving the network for future use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYKEICYsU-6C"
      },
      "source": [
        "## uncomment the following lines if you want to update your model\n",
        "CNN.save(path+'saved_model/'+NETWORK+'_model')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBFZbKbLU-6D"
      },
      "source": [
        "## Loading the model\n",
        "CNN = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'_model')\n",
        "##Check its architecture\n",
        "CNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjlMW6A5U-6D"
      },
      "source": [
        "## Model Evaluation for DPCNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxiA-rKKU-6D"
      },
      "source": [
        "loss, mae, mse = CNN.evaluate(testImagesX, test_labels, verbose=0)\n",
        "print(\"Testing set Mean Square Error for {}: {:5.2f} \".format(NETWORK,mse))\n",
        "print(\"Testing set Root Mean Square Error for {}: {:5.2f} M_Earth\".format( NETWORK,np.sqrt(mse)))\n",
        "print(\"Testing set Mean Abs Error for {} : {:5.2f} M_Earth \".format(NETWORK,mae))\n",
        "print(\"Testing set Loss for {}: {:5.2f} M_Earth\".format(NETWORK,loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZgGkAXcU-6D"
      },
      "source": [
        "## Implementing the hybrid model, i.e., DPCNet + DPNNet##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r_Br0eGU-6D"
      },
      "source": [
        "DPNNet = dm.DPNNet_build(normed_train_data.shape[1], regress=False)\n",
        "\n",
        "\n",
        "\n",
        "if NETWORK == \"Vanilla\":\n",
        "    CNN_ = dm.build_cnn(X_res, Y_res, 3, regress=False)\n",
        "elif NETWORK == \"ALEXNET\":\n",
        "    CNN_ = ocn.alexnet(X_res, Y_res, 3, regress=False)\n",
        "elif NETWORK == \"VGG\":\n",
        "    CNN_ = ocn.cnn_vgg(X_res, Y_res, 3, regress=False)\n",
        "elif NETWORK == \"RESNET50\":\n",
        "    CNN_ = ocn.ResNet50(X_res, Y_res, 3)\n",
        "\n",
        "combinedInput = concatenate([DPNNet.output,CNN_.output])\n",
        "# our final FC layer head will have two dense layers, the final one being our regression head\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VSGgr5U-6D"
      },
      "source": [
        "## Training the Hybrid Model (DPCNet + DPNNet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grlWaDi_U-6D"
      },
      "source": [
        "epochs=200 ## best was 100\n",
        "init_lr = 1e-4\n",
        "hybrid_model = Model(inputs=[DPNNet.input, CNN_.input], outputs=x)\n",
        "# optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(init_lr,decay=init_lr /200) #, \n",
        "hybrid_model.compile(loss='mean_squared_error',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['mean_absolute_error', 'mean_squared_error'])\n",
        "print('INFO: Currently training using the {} NETWORK and DPNNet'.format(NETWORK))\n",
        "history_hybrid = hybrid_model.fit(x=[normed_train_data, trainImagesX], y=train_labels,\n",
        "    validation_split = 0.15,verbose=1,\n",
        "    epochs=epochs, batch_size=batch_size,callbacks=[early_stop])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXOgQmOpU-6D"
      },
      "source": [
        "dp.plot_history(history_hybrid,path, Model = \"Hybrid\")\n",
        "hist_hybrid = pd.DataFrame(history_hybrid.history) \n",
        "hist_hybrid.to_csv(path+'data_folder/'+NETWORK+'history_hybrid.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdklJuAVU-6E"
      },
      "source": [
        "\n",
        "hybrid_model.save(path+'saved_model/'+NETWORK+'_hybrid_model') \n",
        "# hybrid_model = tf.keras.models.load_model(path+'saved_model/'+NETWORK+'hybrid_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz9hTCVpU-6E"
      },
      "source": [
        "## Model Evaluation for Hybrid Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpSf9KsqU-6E"
      },
      "source": [
        "loss, mae, mse = hybrid_model.evaluate([normed_test_data,testImagesX], test_labels, verbose=0)\n",
        "print(\"Testing set Mean Square Error for {} with DPNNet: {:5.2f} \".format(NETWORK,mse))\n",
        "print(\"Testing set Root Mean Square Error for {} with DPNNet: {:5.2f} M_Earth\".format( NETWORK,np.sqrt(mse)))\n",
        "print(\"Testing set Mean Abs Error for {} with DPNNet: {:5.2f} M_Earth \".format(NETWORK,mae))\n",
        "print(\"Testing set Loss for {} with DPNNet: {:5.2f} M_Earth\".format(NETWORK,loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNJ9EL45U-6E"
      },
      "source": [
        "## Model Predictions and Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWW-yJoDU-6E"
      },
      "source": [
        "pred_CNN = CNN.predict(testImagesX)\n",
        "np.shape(pred_CNN)\n",
        "\n",
        "pred_Hybird = hybrid_model.predict([normed_test_data,testImagesX])\n",
        "np.shape(pred_Hybird)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJUHfl8vU-6E"
      },
      "source": [
        "plt.style.use('classic')\n",
        "plt.figure(figsize = (5,5))\n",
        "# test_predictions = model.predict(normed_test_data).flatten()\n",
        "plt.scatter(test_labels,pred_CNN.flatten(),s=30,marker='d',color='r')\n",
        "\n",
        "score_CNN = r2_score(test_labels,pred_CNN.flatten())\n",
        "plt.text(20,110,r\" r2 = {:.3f}\".format(score_CNN), fontsize =14)\n",
        "plt.xlabel(r'True values of planet mass($M_\\oplus$)', fontsize=15)\n",
        "plt.ylabel(r'Predicted planet mass($M_\\oplus$)',fontsize=15)\n",
        "plt.title(\"DPCNet Prediction\")\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim(5,125)\n",
        "plt.ylim(5,125)\n",
        "\n",
        "# plt.xlim([0.6,plt.xlim()[1]])\n",
        "# plt.ylim([0.6,plt.xlim()[1]])\n",
        "_ = plt.plot([0, 120], [0, 120],linewidth=2)\n",
        "plt.minorticks_on() \n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/predicted_correlation_CNN.pdf',format='pdf',dpi=300)\n",
        "\n",
        "plt.tick_params(labelsize=15)\n",
        "\n",
        "plt.tick_params(axis='both', which='major',length=6, width=2)\n",
        "plt.tick_params(axis='both', which='minor',length=3, width=1.3)\n",
        "plt.figure(figsize = (5,5))\n",
        "score_HYBRID = r2_score(test_labels,pred_Hybird.flatten())\n",
        "plt.text(20,110,r\"r2 = {:.3f}\".format(score_HYBRID),fontsize =14)\n",
        "plt.scatter(test_labels,pred_Hybird.flatten(),s=20,marker='d',color='r')\n",
        "plt.title(\"Hybrid Prediction\")\n",
        "plt.xlabel(r'True values of planet mass($M_\\oplus$)', fontsize=15)\n",
        "plt.ylabel(r'Predicted planet mass($M_\\oplus$)',fontsize=15)\n",
        "plt.axis('equal')\n",
        "plt.axis('square')\n",
        "plt.xlim(5,125)\n",
        "plt.ylim(5,125)\n",
        "\n",
        "# plt.xlim([0.6,plt.xlim()[1]])\n",
        "# plt.ylim([0.6,plt.xlim()[1]])\n",
        "_ = plt.plot([0, 120], [0, 120],linewidth=2)\n",
        "\n",
        "plt.minorticks_on() \n",
        "plt.tick_params(labelsize=15)\n",
        "plt.tick_params(axis='both', which='major',length=6, width=2)\n",
        "plt.tick_params(axis='both', which='minor',length=3, width=1.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('figures/predicted_correlation_hybrid.pdf',format='pdf',dpi=300)\n",
        "# print(\"{} r2 score is {}\".format(NETWORK,score_CNN))\n",
        "# print(\"{} + DPPNET r2 score is {}\".format(NETWORK,score_HYBRID))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g804Rdt8U-6E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}